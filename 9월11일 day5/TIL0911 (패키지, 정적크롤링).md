# TIL0911 (패키지, 정적크롤링)

## (1) 패키지

> R 패키지는 R로 할 수 있는 통계, 분석, 시각화와 관련하여 기능을 정의한 함수들의 묶음이다.
>
> R 패키지는 CRAN(https://cran.r-project.org/) 사이트에서 모두 검색가능하고 다운로드할 수 있다.

```R
# 새로운 R 패키지 설치
install.packages("패키지명")

# 이미 설치된 R 패키지 확인
installed.packages()
library()

# 설치된 패키지 삭제
remove.packages("패키지명")

# 설치된 패키지의 버전 확인
packageVersion("패키지명")

# 설치된 패키지 업데이트
update.packages("패키지명")

# 설치된 패키지 로드
library(패키지명)
require(패키지명)

# 로드된 패키지 언로드
detach("package:패키지명")

# 로드된 패키지 점검
search()
```



```R
# R 패키지
library() # 설치된 패키지 리스트
installed.packages() # 설치된 패키지 리스트(콘솔창에서 확인)
search() # 로드된 패키지 리스트(항상 자동 로드되는 패키지들이 있다)

read_excel()
install.packages("readxl")
install.packages("rvest") 
install.packages("XML")
install.packages("httr")
install.packages("readr")

library(readxl) # require(readxl)
excel_data_ex <- read_excel("data/data_ex.xlsx")
getwd()
View(excel_data_ex)
search()
str(excel_data_ex) # tibble(개선된 데이터프레임)
```



## (2) 정적 스크래핑(크롤링)

> **웹 스크래핑(web scraping)** : 웹 사이트 상에서 원하는 부분에 위치한 정보를 컴퓨터로 하여금 자동으로 추출하여 수집하는 기술
>
> **웹 크롤링(web crawling)** :  자동화 봇(bot)인 웹 크롤러가 정해진 규칙에 따라 복수 개의 웹 페이지를 브라우징 하는 행위



- <span style="color:red;">XPath</span> 
  - XML 문서의 일부분을 선택하고 처리하기 위해 만들어진 언어
  - XPath는 XML 문서를 탐색하기 위해 **경로 표현식**을 사용한다
  - `/wikimedia/projects/project/@name`

```xml
<!-- XML 예제 문서 -->
<?xml version="1.0" encoding="utf-8"?>
<wikimedia>
  <projects>
    <project name="Wikipedia" launch="2001-01-05">
      <editions>
        <edition language="English">en.wikipedia.org</edition>
        <edition language="German">de.wikipedia.org</edition>
        <edition language="French">fr.wikipedia.org</edition>
        <edition language="Polish">pl.wikipedia.org</edition>
      </editions>
    </project>
    <project name="Wiktionary" launch="2002-12-12">
      <editions>
        <edition language="English">en.wiktionary.org</edition>
        <edition language="French">fr.wiktionary.org</edition>
        <edition language="Vietnamese">vi.wiktionary.org</edition>
        <edition language="Turkish">tr.wiktionary.org</edition>
      </editions>
    </project>
  </projects>
</wikimedia>
```



- css 선택자로 접근하면 하위 컨텐트 내용을 구분해서 가져오기 어렵다
- 따라서 이렇게 원하는 컨텐트 내용을 가져오려는 경우 XPath를 활용한다 
- //tag5//tag6/tag7 : 조상이 누구이던간에 tag5를 찾고 그 밑에 누가있던간에 tag6/tag7를 찾는다



---



> [개발자도구 F12] - [Copy] - [Copy selector]
>
> #old_content > table > tbody > tr:nth-child(2) > td.title > a.movie.color_b
>
> #old_content > table > tbody > tr:nth-child(3) > td.title > a.movie.color_b
>
> -------------------------------------------> .movie
>
> 
>
> [개발자도구 F12] - [Copy] - [Copy selector]
>
> #old_content > table > tbody > tr:nth-child(2) > td.title > div > em
>
> -------------------------------------------> td.title em
>
> 
>
> [개발자도구 F12] - [Copy] - [Copy XPath]
>
> //*[@id="old_content"]/table/tbody/tr[2]/td[2]/a[1]



---



```R
##### 정적 웹 크롤링과 스크래핑 #####
# 돔객체의 태그 내용과 태그 속성값 가져오기
library(rvest)

url <- "http://unico2013.dothome.co.kr/crawling/tagstyle.html"
text <- read_html(url); text

nodes <- html_nodes(text, "div"); nodes
title <- html_text(nodes); title

node1 <- html_nodes(text, "div:nth-of-type(1)"); node1 # 첫번째 div 태그의 돔객체를 가져옴
html_text(node1) # 태그 컨텐트를 꺼내온다
html_attr(node1, "style") # 태그 속성을 꺼내온다

node2 <- html_nodes(text, "div:nth-of-type(2)"); node2
html_text(node2)
html_attr(node2, "style")

node3 <- html_nodes(text, "div:nth-of-type(3)"); node3
html_text(node3)


##### 영화리뷰 크롤링 #####
# 단일 페이지(rvest 패키지 사용)
text<- NULL; title<-NULL; point<-NULL; review<-NULL; page=NULL
url<- "http://movie.naver.com/movie/point/af/list.nhn?page=1"
text <- read_html(url, encoding="CP949"); text # CP949 = "charset=euc-kr"

# 영화제목
nodes <- html_nodes(text, ".movie")
title <- html_text(nodes); title

# 영화평점
nodes <- html_nodes(text, ".title em")
point <- html_text(nodes); point

# 영화리뷰 
nodes <- html_nodes(text, xpath="//*[@id='old_content']/table/tbody/tr/td[2]/text()")
nodes <- html_text(nodes, trim=TRUE); nodes
review <- nodes[nchar(nodes) > 0]; review
page <- data.frame(title, point, review)
write.csv(page, "movie_reviews.csv")

```

